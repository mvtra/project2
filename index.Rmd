---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Mindy Tran, mt37643

### Introduction 

  Death by drug overdose is a problem that continues to take the lives of many individuals each day. Nearly 70% of all overdoses involves opioids (CDC). The dispensing of opioids is a topic that I am heavily interested in as it has become a prevalent issue in the healthcare industry. Roughly 24% of individuals prescribed opioids for chronic pain misuse them (CDC). In this project, two datasets obtained from the CDC will be used for analysis. The first dataset, called 'overdose', features data from 2019 outlining drug overdose death rates in the US. The second dataset, called 'opioid', includes data from 2019 on opioid dispensing rates, through prescription, in the US. 
  
  In the 'overdose' dataset, there are 4 variables. The variable 'Location' features the 50 states and D.C. The variable 'Age.adjusted.Rate' is the age adjusted rate of deaths by drug overdose per 100,000 population. The variable 'Range.Category' features the minimum to maximum age adjusted rate that was calculated. The variable 'Number.of.Deaths' is the number of drug overdose deaths per 100,000 population. In the 'opioid' dataset, there are three variables. The variable 'States' features the 50 states, D.C. and the U.S. The variable 'Abbreviation' is the abbreviation of each state. The variable 'Opioid.Dispensing.Rate.per.100' is the opioid dispensing rate per 100 people. A binary variable was also created to determine if the opioid dispensing rate, for each location, was greater than 50. All of the variables have 51 observations.

```{R}
library(tidyverse)

overdose <- read.csv('overdose.csv')
opioid <- read.csv('opioid dispensing .csv')

inner_join(overdose,opioid,by=c("Location"="State")) -> opioid_overdose
opioid_overdose %>% mutate(Opioid.Dispensing.Rate.over.50 = Opioid.Dispensing.Rate.per.100>50) -> opioid_overdose
```

### Cluster Analysis

```{R}
library(cluster)

clust_dat <- opioid_overdose %>% select(Number.of.Deaths,Opioid.Dispensing.Rate.per.100,Age.adjusted.Rate)

sil_width<-vector()
for(i in 2:10){  
  pam_fit <- pam(clust_dat, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)

pam1 <- clust_dat %>% pam(k=2)
pam1

library(GGally)
clust_dat %>% mutate(cluster=as.factor(pam1$clustering)) %>% ggpairs(aes(color=cluster))
```

PAM clustering was performed with the numeric variables: number of deaths, opioid dispensing rate, and the age-adjusted rate. Two clusters were picked based on the largest average silhouette width, which was around 0.65. The number of drug overdose deaths per 100,000 population shows the greatest difference between the two clusters. The opioid dispensing rate per 100 people shows the least difference between the two clusters. In cluster 1 (red), the number of drug overdose deaths per 100,000 population is low, along with the opioid dispensing rate and age adjusted rate. In cluster 2 (blue), the number of drug overdose deaths per 100,000 population and the age adjusted rate are high, but the opioid dispensing rate is low. The cluster solution is reasonable as the overall average silhouette width is between than 0.51 - 0.70. 
    
    
### Dimensionality Reduction with PCA

```{R}
library(factoextra)

opioidoverdose_nums <- opioid_overdose %>% select_if(is.numeric)
princomp(opioidoverdose_nums, cor=T) -> pca1

eigval <-  pca1$sdev^2
varprop=round(eigval/sum(eigval), 2)

summary(pca1, loadings=T)

pca1df <-data.frame(PC1=pca1$scores[,1],PC2=pca1$scores[,2])

ggplot(pca1df,aes(PC1,PC2)) + geom_point()

fviz_pca_biplot(pca1)
```

Component 1 features the age-adjusted rate and number of deaths aganist the opioid dispensing rate. Component 2 feature the age-adjusted rate and the opioid dispensing rate. Component 3 feature the age-adjusted rate against the number of deaths and the opioid dispensing rate. In component 1, a high score means the location had a high number of drug overdose deaths and a high age adjusted rate of deaths by drug overdose but had a low opioid dispensing rate. In component 2, a high score means the location had a high age adjusted rate of deaths by drug overdose and a high opioid dispensing rate. In component 3, a high score means the location had a high age adjusted rate of deaths by drug overdose but had a low opioid dispensing rate and a low number of deaths. Component 1 and 2 accounted from seventy-five percent of the total variability. 

###  Linear Classifier

```{R}
# linear classifier code here
logistic_fit <- glm(Opioid.Dispensing.Rate.over.50 == "TRUE" ~ Age.adjusted.Rate + Number.of.Deaths, data=opioid_overdose, family="binomial")


prob_reg <- predict(logistic_fit,type='response')
class_diag(prob_reg, opioid_overdose$Opioid.Dispensing.Rate.over.50, positive= "TRUE")

# confusion matrix
table(truth= opioid_overdose$Opioid.Dispensing.Rate.over.50,
      prediction= prob_reg>.5) %>% addmargins
```

```{R}
# cross-validation of linear classifier here
set.seed(322)
k=10

data<-sample_frac(opioid_overdose)
folds <- rep(1:k, length.out=nrow(data))

diags<-NULL

i=1
for(i in 1:k){

train<-data[folds!=i,] 
test<-data[folds==i,] 
truth<-test$Opioid.Dispensing.Rate.over.50

fit <- glm(Opioid.Dispensing.Rate.over.50 ~ Age.adjusted.Rate + Number.of.Deaths, data=opioid_overdose, family="binomial") 

probs <- predict(fit, newdata=test, type="response")

diags<-rbind(diags,class_diag(probs,truth,positive="TRUE")) }

summarize_all(diags,mean)
```

With an AUC of 0.531, the linear regression model is performing pretty poorly. The cross validation is also performing pretty poorly as it has an AUC of 0.558; the model is predicting new observations per CV AUC badly. Though, there are no real signs of overfitting as the AUC of the cross validation is around 0.02 higher than the AUC of the linear regression.

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
knn_fit <- knn3(Opioid.Dispensing.Rate.over.50 == "TRUE" ~ Age.adjusted.Rate + Number.of.Deaths, data=opioid_overdose)

prob_knn <- predict(knn_fit, opioid_overdose)[,2]

class_diag(prob_knn, opioid_overdose$Opioid.Dispensing.Rate.over.50, positive= "TRUE")

### confusion matrix
table(truth= opioid_overdose$Opioid.Dispensing.Rate.over.50,
      prediction= prob_knn>.5)
```

```{R}
# cross-validation of np classifier here
set.seed(322)
k=10

data<-sample_frac(opioid_overdose)
folds <- rep(1:k, length.out=nrow(data))

diags<-NULL

i=1
for(i in 1:k){

train<-data[folds!=i,] 
test<-data[folds==i,] 
truth<-test$Opioid.Dispensing.Rate.over.50

fit <- knn3(Opioid.Dispensing.Rate.over.50 == "TRUE" ~ Age.adjusted.Rate + Number.of.Deaths, data=opioid_overdose)

probs <- predict(fit,newdata = test)[,2]

diags<-rbind(diags,class_diag(probs,truth,positive="TRUE")) }

summarize_all(diags,mean)
```

The k-nearest-neighbors model is performing slightly better, though not great, as it has an AUC of 0.704. With an AUC of 0.673, the cross validation is also performing slightly better, though it is still not great. There is very little signs of overfitting, not anything meaningful, as the AUC of the k-nearest-neighbors is around 0.03 higher than the AUC of the cross validation. The k-nearest-neighbors model performs fairly better than the linear regression model in its cross-validation performance as the k-nearest-neighbors model has a higher AUC value.


### Regression/Numeric Prediction

```{R}
# regression model code here
fit<-lm(Number.of.Deaths ~ Age.adjusted.Rate + Opioid.Dispensing.Rate.per.100 + Opioid.Dispensing.Rate.over.50, data=opioid_overdose)

yhat<-predict(fit) 

mean((opioid_overdose$Number.of.Deaths-yhat)^2)
```

```{R}
# cross-validation of regression model here
set.seed(322)
k=10

data<-opioid_overdose[sample(nrow(opioid_overdose)),]
folds<-cut(seq(1:nrow(opioid_overdose)),breaks=k,labels=F)
diags<-NULL

for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  
  fit<-lm(Number.of.Deaths~Age.adjusted.Rate+Opioid.Dispensing.Rate.per.100+Opioid.Dispensing.Rate.over.50,data=train)
  yhat<-predict(fit,newdata=test)
  diags[i]<-mean((test$Number.of.Deaths-yhat)^2) 
}

mean(diags)
```

The average mean-squared error in the regression model is 1820798, while the average mean-squared error in the cross validation is 2043487. Since the mean-squared error is higher in cross validation than in the regression model, there are signs of overfitting. 

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3")


avg_death <- "Average Number of Deaths:"
```

```{python}
# python code here
import pandas as pd

overdose=pd.read_csv("overdose.csv",index_col=0)
opioid_dispensing=pd.read_csv("opioid dispensing .csv",index_col=0)

mean_death = overdose["Number of Deaths"].mean()

mean_opioid = opioid_dispensing["Opioid Dispensing Rate per 100"].mean()

print(r.avg_death, mean_death)
```

```{R}
avg_opioid <- "Average Opioid Dispensing Rate:"
cat(c(avg_opioid,py$mean_opioid))
```

R was used to create labels for the average values of opioid dispensing rate and number of deaths, and python was used to define the average values of opioid dispensing rate and number of death. Using "r." and "py$", the objects were shared between R and python.
